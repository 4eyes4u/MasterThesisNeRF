@article{relu,
  author={Kunihiko Fukushima},
  year={1975},
  title={{Cognitron: A self-organizing multilayered neural network}},
  journal={Biological Cybernetics},
  volume={20},
  number={},
  pages={121--136},
  publisher={Springer}
}

@article{sgd,
  author={J. Kiefer and J. Wolfowitz},
  year={1952},
  title={{Stochastic Estimation of the Maximum of a Regression Function}},
  journal={The Annals of Mathematical Statistics},
  pages={462--466},
  volume={23},
  publisher={Institute of Mathematical Statistics}
}

@conference{universal-approx,
  title={{Minimum Width for Universal Approximation}},
  author={Sejun Park, Chulhee Yun, Jaeho Lee and Jinwoo Shin},
  booktitle={Proceedings of American Federation of
             Information Processing Societies: 1977
             National Computer Conference},
  year={2021},
  publisher={ICLR}
}

@inproceedings{nerf,
  title={{NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis}},
	author={Ben Mildenhall and Pratul P. Srinivasan
	and Matthew Tancik and Jonathan T. Barron
	and Ravi Ramamoorthi and Ren Ng},
	year={2020},
	booktitle={ECCV},
}

@article{instant-ngp,
	author={Thomas M\"uller and Alex Evans and
	Christoph Schied and Alexander Keller},
  title={{Instant Neural Graphics Primitives with a Multiresolution Hash Encoding}},
	journal={ACM Trans. Graph.},
	issue_date={July 2022},
	volume={41},
	number={4},
	month=jul,
	year={2022},
	pages={102:1--102:15},
	articleno={102},
	numpages={15},
	url={https://doi.org/10.1145/3528223.3530127},
	doi={10.1145/3528223.3530127},
	publisher={ACM},
	address={New York, NY, USA},
}

@article{mip-nerf,
  title={{Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields}},
	author={Jonathan T. Barron and Ben Mildenhall and 
	Matthew Tancik and Peter Hedman and 
	Ricardo Martin-Brualla and Pratul P. Srinivasan},
	journal={ICCV},
	year={2021}
}

@incollection{pytorch,
  title={{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
	author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury,
		James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein,
		Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang,
		Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy,
		Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	booktitle={Advances in Neural Information Processing Systems 32},
	pages={8024--8035},
	year={2019},
	publisher={Curran Associates, Inc.},
	url={http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{imagenet,
  title={{Imagenet: A large-scale hierarchical image database}},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{ssim,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={{Image quality assessment: from error visibility to structural similarity}}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  doi={10.1109/TIP.2003.819861}}

@inproceedings{lpips,
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={{The Unreasonable Effectiveness of Deep Features as a Perceptual Metric}}, 
  year={2018},
  volume={},
  number={},
  pages={586-595},
  doi={10.1109/CVPR.2018.00068}
}

@inproceedings{alexnet,
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  editor={F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages={},
  publisher={Curran Associates, Inc.},
  title={{ImageNet Classification with Deep Convolutional Neural Networks}},
  url={https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume={25},
  year={2012}
}

@inproceedings{vgg,
  author={Karen Simonyan and Andrew Zisserman},
  title={{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@article{transformer,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
  title={{Attention is All You Need}},
  year = {2017},
  isbn = {9781510860964},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  journal = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages = {6000–6010},
  numpages = {11},
  location = {Long Beach, California, USA},
  series = {NIPS'17}
}

@article{fourier-coefficients,
  title={{Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains}},
  author={Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
  journal={NeurIPS},
  year={2020}
}

@inproceedings{volume-rendering,
  title={{Local and Global Illumination in the Volume Rendering Integral}},
  author={Nelson L. Max and Min Chen},
  booktitle={Scientific Visualization: Advanced Concepts},
  year={2010}
}

@article{mip-nerf,
  title={{Mip-NeRF: A Multiscale Representation  for Anti-Aliasing Neural Radiance Fields}},
  author={Jonathan T. Barron and Ben Mildenhall and Matthew Tancik and Peter Hedman and 
          Ricardo Martin-Brualla and Pratul P. Srinivasan},
  journal={ICCV},
  year={2021}
}

@book{probabilistic-approach,
  address={Cambridge, Mass. [u.a.]},
  author={Murphy, Kevin P.},
  isbn={9780262018029 0262018020},
  publisher={MIT Press},
  refid={904442949},
  timestamp={2017-02-27T11:22:42.000+0100},
  title={{Machine learning: a probabilistic perspective}},
  year=2013
}

@article{ref-nerf,
  title={{{Ref-NeRF}}: Structured View-Dependent Appearance for
          Neural Radiance Fields},
  author={Dor Verbin and Peter Hedman and Ben Mildenhall and
          Todd Zickler and Jonathan T. Barron and Pratul P. Srinivasan},
  journal={CVPR},
  year={2022}
}

@article{sgrb,
  title={{Proposal for a Standard Default Color Space for the Internet—sRGB}},
  author={Matthew Anderson and Ricardo Motta and Srinivasan Chandrasekar and
          Michael Stokes},
  journal={International Conference on Communications in Computing},
  year={1996}
}

@inproceedings{spectral-analysis,
  title={{On the Spectral Bias of Neural Networks}},
  author =       {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5301--5310},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/rahaman19a.html},
  abstract = 	 {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.}
}

@inproceedings{psnr,
  author={Horé, Alain and Ziou, Djemel},
  booktitle={2010 20th International Conference on Pattern Recognition}, 
  title={{Image Quality Metrics: PSNR vs. SSIM}}, 
  year={2010},
  volume={},
  number={},
  pages={2366-2369},
  doi={10.1109/ICPR.2010.579}}